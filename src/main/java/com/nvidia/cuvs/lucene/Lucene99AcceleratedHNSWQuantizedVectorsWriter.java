/*
 * SPDX-FileCopyrightText: Copyright (c) 2025-2026, NVIDIA CORPORATION.
 * SPDX-License-Identifier: Apache-2.0
 */
package com.nvidia.cuvs.lucene;

import static com.nvidia.cuvs.lucene.AcceleratedHNSWUtils.cagraIndexParams;
import static com.nvidia.cuvs.lucene.AcceleratedHNSWUtils.createMultiLayerHnswGraph;
import static com.nvidia.cuvs.lucene.AcceleratedHNSWUtils.createSingleVectorHnswGraph;
import static com.nvidia.cuvs.lucene.AcceleratedHNSWUtils.printInfoStream;
import static com.nvidia.cuvs.lucene.AcceleratedHNSWUtils.writeEmpty;
import static com.nvidia.cuvs.lucene.AcceleratedHNSWUtils.writeGraph;
import static com.nvidia.cuvs.lucene.AcceleratedHNSWUtils.writeMeta;
import static com.nvidia.cuvs.lucene.Lucene99AcceleratedHNSWVectorsFormat.HNSW_INDEX_CODEC_NAME;
import static com.nvidia.cuvs.lucene.Lucene99AcceleratedHNSWVectorsFormat.HNSW_INDEX_EXT;
import static com.nvidia.cuvs.lucene.Lucene99AcceleratedHNSWVectorsFormat.HNSW_META_CODEC_EXT;
import static com.nvidia.cuvs.lucene.Lucene99AcceleratedHNSWVectorsFormat.HNSW_META_CODEC_NAME;
import static com.nvidia.cuvs.lucene.ThreadLocalCuVSResourcesProvider.closeCuVSResourcesInstance;
import static com.nvidia.cuvs.lucene.ThreadLocalCuVSResourcesProvider.getCuVSResourcesInstance;
import static org.apache.lucene.index.VectorEncoding.BYTE;
import static org.apache.lucene.index.VectorEncoding.FLOAT32;
import static org.apache.lucene.search.DocIdSetIterator.NO_MORE_DOCS;
import static org.apache.lucene.util.RamUsageEstimator.shallowSizeOfInstance;

import com.nvidia.cuvs.CagraIndex;
import com.nvidia.cuvs.CagraIndexParams;
import com.nvidia.cuvs.CuVSMatrix;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Objects;
import org.apache.lucene.codecs.CodecUtil;
import org.apache.lucene.codecs.KnnFieldVectorsWriter;
import org.apache.lucene.codecs.KnnVectorsWriter;
import org.apache.lucene.codecs.hnsw.FlatFieldVectorsWriter;
import org.apache.lucene.codecs.hnsw.FlatVectorsWriter;
import org.apache.lucene.codecs.lucene99.Lucene99HnswVectorsFormat;
import org.apache.lucene.index.DocsWithFieldSet;
import org.apache.lucene.index.FieldInfo;
import org.apache.lucene.index.FloatVectorValues;
import org.apache.lucene.index.IndexFileNames;
import org.apache.lucene.index.KnnVectorValues;
import org.apache.lucene.index.MergeState;
import org.apache.lucene.index.SegmentWriteState;
import org.apache.lucene.index.Sorter;
import org.apache.lucene.index.Sorter.DocMap;
import org.apache.lucene.store.IndexOutput;
import org.apache.lucene.util.IOUtils;
import org.apache.lucene.util.InfoStream;

/**
 * This class extends upon the KnnVectorsWriter to enable the creation of GPU-based accelerated
 * vector search indexes.
 *
 * @since 26.02
 */
public class Lucene99AcceleratedHNSWQuantizedVectorsWriter extends KnnVectorsWriter {

  private static final long SHALLOW_RAM_BYTES_USED =
      shallowSizeOfInstance(Lucene99AcceleratedHNSWQuantizedVectorsWriter.class);

  /** The name of the CUVS component for the info-stream * */
  private static final String COMPONENT = "Lucene99AcceleratedHNSWQuantizedVectorsWriter";

  private final int cuvsWriterThreads;
  private final int intGraphDegree;
  private final int graphDegree;
  private final int hnswLayers;
  private final FlatVectorsWriter flatVectorsWriter;
  private final List<ScalarQuantizedGPUFieldWriter> fields = new ArrayList<>();
  private final InfoStream infoStream;
  private IndexOutput hnswMeta = null, hnswVectorIndex = null;
  private boolean finished;
  private String vemFileName;
  private String vexFileName;

  /**
   * Initializes {@link Lucene99AcceleratedHNSWQuantizedVectorsWriter}
   *
   * @param state instance of the {@link org.apache.lucene.index.SegmentWriteState}
   * @param cuvsWriterThreads number of cuVS threads to use while building the intermediate CAGRA index
   * @param intGraphDegree the intermediate graph degree to use while building the CAGRA index
   * @param graphDegree the graph degree to use while building the CAGRA index
   * @param hnswLayers the number of hnsw layers to construct while building the HNSW graph
   * @param flatVectorsWriter instance of the {@link org.apache.lucene.codecs.hnsw.FlatVectorsWriter}
   * @throws IOException IOException
   */
  public Lucene99AcceleratedHNSWQuantizedVectorsWriter(
      SegmentWriteState state,
      int cuvsWriterThreads,
      int intGraphDegree,
      int graphDegree,
      int hnswLayers,
      FlatVectorsWriter flatVectorsWriter)
      throws IOException {
    super();
    this.cuvsWriterThreads = cuvsWriterThreads;
    this.intGraphDegree = intGraphDegree;
    this.graphDegree = graphDegree;
    this.hnswLayers = hnswLayers;
    this.flatVectorsWriter = flatVectorsWriter;
    this.infoStream = state.infoStream;

    vemFileName =
        IndexFileNames.segmentFileName(
            state.segmentInfo.name, state.segmentSuffix, HNSW_META_CODEC_EXT);

    vexFileName =
        IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, HNSW_INDEX_EXT);

    boolean success = false;
    try {

      hnswMeta = state.directory.createOutput(vemFileName, state.context);
      hnswVectorIndex = state.directory.createOutput(vexFileName, state.context);

      CodecUtil.writeIndexHeader(
          hnswMeta,
          HNSW_META_CODEC_NAME,
          Lucene99HnswVectorsFormat.VERSION_CURRENT,
          state.segmentInfo.getId(),
          state.segmentSuffix);
      CodecUtil.writeIndexHeader(
          hnswVectorIndex,
          HNSW_INDEX_CODEC_NAME,
          Lucene99HnswVectorsFormat.VERSION_CURRENT,
          state.segmentInfo.getId(),
          state.segmentSuffix);
      success = true;
      printInfoStream(
          infoStream, COMPONENT, "Lucene99AcceleratedHNSWQuantizedVectorsWriter opened");
    } finally {
      if (success == false) {
        IOUtils.closeWhileHandlingException(this);
      }
    }
  }

  /**
   * Add new field for indexing.
   */
  @Override
  public KnnFieldVectorsWriter<?> addField(FieldInfo fieldInfo) throws IOException {
    var encoding = fieldInfo.getVectorEncoding();
    var writer = Objects.requireNonNull(flatVectorsWriter.addField(fieldInfo));

    if (encoding == FLOAT32) {
      @SuppressWarnings("unchecked")
      var flatWriter = (FlatFieldVectorsWriter<float[]>) writer;
      var cuvsFieldWriter = new ScalarQuantizedGPUFieldWriter(fieldInfo, flatWriter);
      fields.add(cuvsFieldWriter);
      return writer;
    } else if (encoding == BYTE) {
      @SuppressWarnings("unchecked")
      var flatWriter = (FlatFieldVectorsWriter<byte[]>) writer;
      var cuvsFieldWriter = new ScalarQuantizedGPUFieldWriter(fieldInfo, flatWriter);
      fields.add(cuvsFieldWriter);
      return writer;
    } else {
      throw new IllegalArgumentException(
          "expected FLOAT32 or BYTE encoding for scalar quantized vectors, got:" + encoding);
    }
  }

  /**
   * Utility method for building index metadata information string object.
   *
   * @param size index size
   * @param args additional metadata information
   * @return the string representation of the metadata information
   */
  static String indexMsg(int size, int... args) {
    StringBuilder sb = new StringBuilder("cagra index params");
    sb.append(": size=").append(size);
    sb.append(", intGraphDegree=").append(args[0]);
    sb.append(", actualIntGraphDegree=").append(args[1]);
    sb.append(", graphDegree=").append(args[2]);
    sb.append(", actualGraphDegree=").append(args[3]);
    return sb.toString();
  }

  private static byte signedToUnsignedByte(byte signedByte) {
    return (byte) (signedByte & 0xFF);
  }

  private static byte[] convertSignedToUnsigned(byte[] signedVector) {
    byte[] unsignedVector = new byte[signedVector.length];
    for (int i = 0; i < signedVector.length; i++) {
      unsignedVector[i] = signedToUnsignedByte(signedVector[i]);
    }
    return unsignedVector;
  }

  /**
   * Builds the intermediate CAGRA index and builds and writes the HNSW index.
   *
   * @param fieldInfo instance of FieldInfo that has the field description
   * @param vectors quantized vectors
   * @throws IOException
   */
  private void writeFieldInternal(FieldInfo fieldInfo, List<byte[]> vectors) throws IOException {
    if (vectors.size() == 0) {
      writeEmpty(fieldInfo, hnswMeta);
      return;
    }

    try {
      int dimensions = fieldInfo.getVectorDimension();

      // Convert 7-bit signed bytes to 8-bit unsigned bytes for cuVS compatibility
      List<byte[]> unsignedVectors = new ArrayList<>(vectors.size());
      for (byte[] signedVector : vectors) {
        unsignedVectors.add(convertSignedToUnsigned(signedVector));
      }

      // Create CuVSMatrix with BYTE data type (unsigned bytes)
      CuVSMatrix dataset =
          Utils.createByteMatrix(unsignedVectors, dimensions, getCuVSResourcesInstance());

      if (dataset.size() < 2) {
        writeSingleVectorGraph(fieldInfo, unsignedVectors);
        return;
      }

      CagraIndexParams params = cagraIndexParams(cuvsWriterThreads, intGraphDegree, graphDegree);
      CagraIndex cagraIndex =
          CagraIndex.newBuilder(getCuVSResourcesInstance())
              .withDataset(dataset)
              .withIndexParams(params)
              .build();

      CuVSMatrix adjacencyListMatrix = cagraIndex.getGraph();

      int size = (int) dataset.size();
      GPUBuiltHnswGraph hnswGraph =
          createMultiLayerHnswGraph(
              fieldInfo,
              size,
              dimensions,
              adjacencyListMatrix,
              unsignedVectors,
              hnswLayers,
              graphDegree,
              params);

      long vectorIndexOffset = hnswVectorIndex.getFilePointer();

      // Write the graph to the vector index
      int[][] graphLevelNodeOffsets = writeGraph(hnswGraph, hnswVectorIndex);

      long vectorIndexLength = hnswVectorIndex.getFilePointer() - vectorIndexOffset;

      // Write metadata
      writeMeta(
          hnswVectorIndex,
          hnswMeta,
          fieldInfo,
          vectorIndexOffset,
          vectorIndexLength,
          size,
          hnswGraph,
          graphLevelNodeOffsets,
          graphDegree);

      cagraIndex.close();
    } catch (Throwable t) {
      Utils.handleThrowable(t);
    }
  }

  /**
   * Build the indexes and writes it to the disk.
   */
  @Override
  public void flush(int maxDoc, DocMap sortMap) throws IOException {
    flatVectorsWriter.flush(maxDoc, sortMap);
    for (var field : fields) {
      if (sortMap == null) {
        writeField(field);
      } else {
        writeSortingField(field, sortMap);
      }
    }
  }

  /**
   * Builds the index and writes it to the disk.
   *
   * @param fieldData
   * @throws IOException
   */
  private void writeField(ScalarQuantizedGPUFieldWriter fieldData) throws IOException {
    writeFieldInternal(fieldData.fieldInfo(), fieldData.getVectors());
  }

  /**
   * Builds the index and writes it to the disk.
   *
   * @param fieldData instance of ScalarQuantizedGPUFieldWriter
   * @param sortMap instance of the DocMap
   * @throws IOException
   */
  private void writeSortingField(ScalarQuantizedGPUFieldWriter fieldData, Sorter.DocMap sortMap)
      throws IOException {

    DocsWithFieldSet oldDocsWithFieldSet = fieldData.getDocsWithFieldSet();
    final int[] new2OldOrd = new int[oldDocsWithFieldSet.cardinality()]; // new ord to old ord
    mapOldOrdToNewOrd(oldDocsWithFieldSet, sortMap, null, new2OldOrd, null);

    List<byte[]> sortedVectors = new ArrayList<byte[]>();
    for (int i = 0; i < fieldData.getVectors().size(); i++) {
      sortedVectors.add(fieldData.getVectors().get(new2OldOrd[i]));
    }

    writeFieldInternal(fieldData.fieldInfo(), sortedVectors);
  }

  /**
   * Builds and writes a single vector graph.
   *
   * @param fieldInfo instance of FieldInfo
   * @param vectors the list of scalar quantized vectors (already converted to unsigned)
   * @throws IOException I/O Exceptions
   */
  private void writeSingleVectorGraph(FieldInfo fieldInfo, List<byte[]> vectors)
      throws IOException {
    // Workaround for CAGRA not supporting single vector indexes
    try {
      int size = 1;
      int dimensions = fieldInfo.getVectorDimension();

      // Create a dummy HNSW graph for a single vector
      GPUBuiltHnswGraph hnswGraph = createSingleVectorHnswGraph(size, dimensions);

      long vectorIndexOffset = hnswVectorIndex.getFilePointer();
      // Write the graph to the vector index
      int[][] graphLevelNodeOffsets = writeGraph(hnswGraph, hnswVectorIndex);
      long vectorIndexLength = hnswVectorIndex.getFilePointer() - vectorIndexOffset;

      // Write metadata
      writeMeta(
          hnswVectorIndex,
          hnswMeta,
          fieldInfo,
          vectorIndexOffset,
          vectorIndexLength,
          size,
          hnswGraph,
          graphLevelNodeOffsets,
          graphDegree);

    } catch (Throwable t) {
      Utils.handleThrowable(t);
    }
  }

  /**
   * Write field for merging.
   */
  @Override
  public void mergeOneField(FieldInfo fieldInfo, MergeState mergeState) throws IOException {
    flatVectorsWriter.mergeOneField(fieldInfo, mergeState);
    vectorBasedMerge(fieldInfo, mergeState);
  }

  /**
   * Fallback method that rebuilds indexes from merged vectors.
   * Used when native CAGRA merge() is not possible.
   */
  private void vectorBasedMerge(FieldInfo fieldInfo, MergeState mergeState) throws IOException {
    try {
      FloatVectorValues mergedVectorValues =
          KnnVectorsWriter.MergedVectorValues.mergeFloatVectorValues(fieldInfo, mergeState);

      if (mergedVectorValues != null) {
        List<float[]> floatVectors = new ArrayList<>();
        KnnVectorValues.DocIndexIterator iter = mergedVectorValues.iterator();
        for (int docV = iter.nextDoc(); docV != NO_MORE_DOCS; docV = iter.nextDoc()) {
          floatVectors.add(mergedVectorValues.vectorValue(iter.index()));
        }

        if (!floatVectors.isEmpty()) {
          int dimensions = floatVectors.get(0).length;
          int numVectors = floatVectors.size();

          float[] minPerDim = new float[dimensions];
          float[] maxPerDim = new float[dimensions];
          Arrays.fill(minPerDim, Float.MAX_VALUE);
          Arrays.fill(maxPerDim, Float.MIN_VALUE);

          for (float[] vector : floatVectors) {
            for (int d = 0; d < dimensions; d++) {
              minPerDim[d] = Math.min(minPerDim[d], vector[d]);
              maxPerDim[d] = Math.max(maxPerDim[d], vector[d]);
            }
          }

          List<byte[]> dataset = new ArrayList<>(numVectors);
          for (float[] vector : floatVectors) {
            byte[] quantized = new byte[dimensions];
            for (int d = 0; d < dimensions; d++) {
              float value = vector[d];
              float min = minPerDim[d];
              float max = maxPerDim[d];

              byte signedByte;
              if (max - min == 0) {
                signedByte = 0;
              } else {
                float normalized = (value - min) / (max - min);
                signedByte = (byte) Math.round(normalized * 127 - 64);
              }

              quantized[d] = signedToUnsignedByte(signedByte);
            }
            dataset.add(quantized);
          }
          writeFieldInternal(fieldInfo, dataset);
        }
      }
    } catch (Throwable t) {
      Utils.handleThrowable(t);
    }
  }

  /**
   * Called once at the end before close.
   */
  @Override
  public void finish() throws IOException {
    if (finished) {
      throw new IllegalStateException("already finished");
    }
    finished = true;
    flatVectorsWriter.finish();
    if (hnswMeta != null) {
      // write end of fields marker
      hnswMeta.writeInt(-1);
      CodecUtil.writeFooter(hnswMeta);
    }
    if (hnswVectorIndex != null) {
      CodecUtil.writeFooter(hnswVectorIndex);
    }
  }

  /**
   * Closes the resources.
   */
  @Override
  public void close() throws IOException {
    IOUtils.close(hnswMeta, hnswVectorIndex, flatVectorsWriter);
    closeCuVSResourcesInstance();
  }

  /**
   * Returns the memory usage of this object in bytes.
   */
  @Override
  public long ramBytesUsed() {
    long total = SHALLOW_RAM_BYTES_USED;
    for (var field : fields) {
      total += field.ramBytesUsed();
    }
    return total;
  }
}
